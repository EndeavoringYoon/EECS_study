{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.2: Image Classification  \n",
    "based on deep learning for computer vision in Michigan Online: https://www.youtube.com/watch?v=0nqvO3AM2Vw&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Algorithm, it takes input as an image and in output, it assigns image to one of a fixed set of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.1)image classification example](pictures/2.0.1.jpg)\n",
    "\n",
    "<center>(2.0.1) image classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main challenge in image recognition and image classification is the probelm we called 'Semantic Gap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic gap characterizes the difference between two descriptions of an object by different linguistic representations. In this case, we use the term semantic gap to introduce difference of visual recognition on image, between human and algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we recognize pircture as itself, not aware of the process like lights goes into our eyes->retina->brain..and processing. But the computer doesn't have these kinds of intuition, and it recognizes an image as a giant grid of numbers between [0, 255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no such evidence that this grid of numbers will represent these semantically meaningful category. And even worse, these entire grid of numbers can drastically change, because of these kinds of challenges:\n",
    "\n",
    "1. Viewpoint variation: difference in viewpoint makes its image to have different set of numbers. So the computer may not recognize correlations between these images.\n",
    "2. Intraclass variation: variance in specific category can also cause difficulties for the computer in classifying images.\n",
    "3. Fine-grained categories: while intraclass variation is a kind of difficulty classifying less-similar images in one category, fine-grained categories are challenges caused by similar images in different category.\n",
    "4. Background clutter: the object we want to classify may not appear clearly because of its background.\n",
    "5. Illumination changes: brightness might change values, challenging the computer to recognize its characteristics.\n",
    "6. deformation: the object we want to recognize may appear in different poses(or conditions).\n",
    "7. Occlusion: the object even may not visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.2) challenges in classification](pictures/2.0.2.jpg)\n",
    "<center>(2.0.2) challenges in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification: Building Block for other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image classification is also a fundamental building block of different algorithms we might want to perform inside computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Object detection: draw boxes around the objects in images and showing where are they located and what are they.\n",
    "\n",
    "    We can use image classification as a sub part to build up more complex applications like object detection.\n",
    "\n",
    "    For example, one way to perform object detection is via image classification of different sliding windows in the image. So one way to perform object detection is just to classify differnet sub-regions of the image.\n",
    "\n",
    "** Image Captioning: Writing a natural language sentece to describe what is in the image\n",
    "    often framed as a sequence of image classification problem\n",
    "\n",
    "    Image classification can be used here as a tool to choose appropriate words for image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.3) building block ](pictures/2.0.3.jpg)\n",
    "\n",
    "<center>(2.0.3) image classification as a building block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An image classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some ways that is more robust or more scalable or ways that doesn't require us to write down all of human knowledge about what different type of objects look like;here's where we come to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Data-Driven Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of encoding all human knowledge, we'll take a data-driven approach and have algorithms that can learn from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the basic pipeline for this machine learning\n",
    "1. Collect a dataset of images and labels\n",
    "    collect images and label them with the types of lables we want our algorithm to predict\n",
    "2. Use Machine Learning to train a classifier\n",
    "    Deploy some kind of machine learning algorithm which will try to learn statistical dependencies between the input images in the dataset and the output labels that we wrote down during the data collection process\n",
    "3. Evaluate the classifier on new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we need two piece API:\n",
    "\n",
    "1. train function:\n",
    "    input collections of images and their associated labels to perform, and return some statistical model\n",
    "2. predict function:\n",
    "    input the model that we produced during the training phase, and new images on which to evaluate that model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to recode our machine learning algorithm even if the images or objects are changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mnist <br/>\n",
    "10 classes: Digits 0 to 9<br/>\n",
    "28*28 grayscale images  <br/>\n",
    "50k training images  <br/>\n",
    "10k test images  <br/>\n",
    "\n",
    "    Take in mind that results from MNIST often do not hold on more complex datasets\n",
    "\n",
    "#### CIFAR10<br/>\n",
    "10 classes: ariplane, automobile, bird,...,etc <br/> \n",
    "32*32 RGB images  <br/>\n",
    "50k training images(5k per class)  <br/>\n",
    "10k testing images(1k per class)  <br/>\n",
    "\n",
    "    Even its size is small, compared to other large-scale datasets, but it is reasonably challenging since these categories are reasonably difficult to recognize\n",
    "\n",
    "#### ImageNet<br/> \n",
    "1000 classes  <br/>\n",
    "variable size, but often resized to 256*256 for training<br/>\n",
    "~1.3M training images(~1.3K per class)  <br/>\n",
    "50k validation images(50 per class)  <br/>\n",
    "100k test images(100 per class)  <br/>\n",
    "\n",
    "    Gold standard for image classification datasets\n",
    "\n",
    "    Performance metric: Top 5 accuracy  \n",
    "    Algorithm predicts 5 labels for each image; one of them needs to be right\n",
    "\n",
    "#### MIT places<br/>\n",
    "365 classes: scene types<br/>\n",
    "variable size, but often resized to 256*256 for training<br/>\n",
    "~8M training images<br/>\n",
    "18.25K validation images(50 per class)<br/>\n",
    "328.5K test images(900 per class)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "complexities of the visual recognition tasks(ImageNet, Places365)<br/>\n",
    "$$|$$  \n",
    "$$|$$  \n",
    "$$CIFAR$$  \n",
    "$$|$$  \n",
    "$$|$$  \n",
    "computational affordability of smaller datasets(MNIST)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification Datasets-other direction\n",
    "\n",
    "It's definitely one interesting direction for research how can we use bigger datasets to enhance abilities of our algorithms to perform robust classification.\n",
    "\n",
    "But people also started thinking in the other direction as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omnigot<br/>\n",
    "1623 categories: characters from 50 different alphabets  \n",
    "20 images per category<br/> \n",
    "\n",
    "    Meant to test few shot learning: when we want to benchmark the ability of algorithms to learn with relatively little data  \n",
    "    \n",
    "    This method, so-called low shot classification problem is a really huge and emergin key area of research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First classifier: Nearest Neighbor\n",
    "\n",
    "We talked about two pieces of API: train and predict.\n",
    "\n",
    "For nearest neighbor, the train function is trivial. it will simply going to memorize all data and labels(no sending, processing,etc).\n",
    "\n",
    "In the predict side, we'll take our new image that we want to predict a label for compare it to each one of our images in the training set using some kind of comparison or similarity function. we'll keep track of the most similar image in the training set to our test image and return the label of the most similar training image.\n",
    "\n",
    "Very straightforward running algorithm, and it learns in the sense that it is kind of memorizes the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metric to compare images\n",
    "\n",
    "We need some functiona that can compute the similarity between two input images. The very common choice is to use Distance metric to compare images which can input a pair of images and then spit out some number representing how sementically similar are those two pairs of images in order to perform this nearest neighbor classification.\n",
    "\n",
    "1. Using L1(Manhattan) distance\n",
    "$$\\mathbf{d_{1}(I_{1},I_{2})} = \\sum_{P} \\left\\lvert{I_{1}^{P}-I_{2}^{P}}\\right\\lvert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.4) L1 distance ](pictures/2.0.4.jpg)\n",
    "\n",
    "<center>(2.0.4) Distance Metric-L1 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NearestNeighbor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    #----------------Train step--------------#\n",
    "    def train(self, X, y):\n",
    "        \"\"\"X is N*D where each row is an example. Y is l-dimension of size N\"\"\"\n",
    "        #the nearest neighbor classifier simply remembers all the training data\n",
    "        self.Xtr = X\n",
    "        self.Ytr = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"X is N*D where each row is an example we wish to predict label for\"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        #let's make sure that the output type matches the input type\n",
    "        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n",
    "\n",
    "\n",
    "    #-----------For each test image: Find nearest train image\n",
    "    #                       return label of min---------------\n",
    "    \n",
    "        #loop over all test rows\n",
    "        for i in xrange(num_test):\n",
    "            #find the nearest training image to the i'th test image\n",
    "            #using the L1 distance (sum of absolute value differences)\n",
    "            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n",
    "            min_index = np.argmin(distances)\n",
    "            Ypred[i] = self.ytr[min_index]\n",
    "        \n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. With N examples, how fast is training?\n",
    "->about constant time\n",
    "2. With N examples, how fast is testing?\n",
    "->linear time\n",
    "\n",
    "*This is bad: we can afford slow training, but we need fast testing*\n",
    "\n",
    "Later, we'll talk about neural-network method, which takes relatively long time on training but relative short time in testing.\n",
    "\n",
    "The result of nearest neighbor is quite different from what we wanted: since this method finds training images that have 'similar pixels' in the test image, the result might not have labels that we expected.\n",
    "\n",
    "But if you have situations when you really need to use this method, take in mind that here are also many methods for fast/approximate nearest neighbors\n",
    "(https://github.com/facebookresearch/faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision boundary is the boundary between two classification regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.5) Decision boundary ](pictures/2.0.5.jpg)\n",
    "\n",
    "<center>(2.0.5) Decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors\n",
    "\n",
    "To make its boundaries smooth and its algorithms to learn robustly, instead of copying label from 'one' nearest neighbor, we can take majority vote from 'K' closest points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.6) K-Nearest Neighbors ](pictures/2.0.6.jpg)\n",
    "\n",
    "<center>(2.0.6) K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors: Distance Metric\n",
    "L1 (Manhattan) distance\n",
    "$$\\mathbf{d_{1}(I_{1},I_{2})} = \\sum_{P} \\left\\lvert{I_{1}^{P}-I_{2}^{P}}\\right\\lvert$$\n",
    "\n",
    "vs.\n",
    "\n",
    "L2 (Euclidean) distance\n",
    "$$\\mathbf{d_{2}(I_{1},I_{2})} =\\sqrt{ \\sum_{P} {(I_{1}^{P}-I_{2}^{P})}^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.7) L1 vs. L2 ](pictures/2.0.7.jpg)\n",
    "![(2.0.7-1) Decision boundaries ](pictures/2.0.7-1.jpg)\n",
    "<center>(2.0.7) L1 vs. L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use different distance metrics we get sort of qualitively different properties in the decision boundaries.\n",
    "\n",
    "With L1 classification, we can see that all of the decision between categories are all composed of access aligned chunks. They're either vertical,horizontal,and 45-degress angle line segments.\n",
    "\n",
    "But when we use L2 distance classification, those lines can appear at any orientation in the input.\n",
    "\n",
    "Using different distance metrics can be a way that human expert can imbue some of your own human knowledge into the structure that you want the algorithm to take account of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the right choice of distance metric, we can apply K-Nearest Neighbor to any type of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.f) K-Nearest Neighbors: Web Demo <br/>\n",
    "http://vision.stanford.edu/teaching/cs231n-demos/knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choices about our learning algorithm that we don't learn from the training data<br/> (ex: in Nearest Neighbor classification, K and distance metric is hyperparameters)\n",
    "\n",
    "Since we cannot set them directly through learning, so we need some other mechanism to choose which values of hyper parameters are going to work fast on our data.\n",
    "\n",
    "There are not a lot of great ways in practice to choose hyper parameters, but one kind of simplest approach is that they're very problem-dependent, so try out different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Hyperparameters\n",
    "\n",
    "1. Idea #1: Choose hyperparameters that work best on the data\n",
    "    we should select the values of hyperparameters that will cause our learning algorithm to give us the highest accuracy on our training set\n",
    "\n",
    "    ->Bad: K = 1 always works perfectly on training data<br/>\n",
    "     highest accuracy on training set doesn't mean same as highest accuracy on test set<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Idea #2: split data into train and test, choose hyperparameters that work best on test data\n",
    "\n",
    "    ->Better than Idea #1, but bad: No idea how algorithm will perform on new data\n",
    "    This idea is a different way of learning on the test set(polluted with knowledge of that test set).\n",
    "    \n",
    "    it makes fundamental error\n",
    "    \n",
    "        c.f.) There is a popular assumption in standard supervised learning, called independent identical distributions assumption(a.k.a. i.i.d. assumption): We assume that training data and test are drawn independently from identical distributions. <br/>\n",
    "        When we use idea #2, it uses test data in train process, which ruins i.i.d. assumption. The popular solution for this is making validation data set, the test set used only for hyperparameter setting(It comes in Idea #3)\n",
    "<br/>\n",
    "\n",
    "3. Idea #3: Split data into train, val, and test; choose hyperparameters on val and evaluate on test\n",
    "\n",
    "    ->Better than Idea #1,#2.\n",
    "\n",
    "<br/>\n",
    "\n",
    "4. Idea #4: Cross-Validation: Split data into folds, try each fold as validation and average the results\n",
    "\n",
    "    ->Most robust way to choose hyper parameters, but because of expense, it doesn't typically get done in most machine learning projects\n",
    "    \n",
    "        c.f.)Also called as K-Fold Cross Validation\n",
    "        Often used when we don't have enough train data to make validation set.  \n",
    "        Devide initial training data into K non-overlapping subsets, and experiment K times with K-1 training subsets and one validation subset.\n",
    "        The training and validation error will be the mean value of K experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.8) setting hyperparameters ](pictures/2.0.8.jpg)\n",
    "<center>(2.0.8) setting hyperparameters\n",
    "\n",
    "![(2.0.8.1) 5-fold cross-validation example ](pictures/2.0.8.1.jpg)\n",
    "<center>(2.0.8.1) 5-fold cross-validation example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor: Universal Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor acutally makes very few assumptions but the types of functions that it can represent.\n",
    "\n",
    "As we take the number of training samples to infinity then K-nearest neighbor can actually represent any function of course\n",
    "\n",
    "(It subjects to many technical conditions, of course. Only continuous functions on a compact domain;need to make assumptions about apcing of training points;etc).\n",
    "\n",
    "ex)continuous valued prediction using a nearest neighbor approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.9) continuous valued prediction ](pictures/2.0.9.jpg)\n",
    "<center>(2.0.9) continuous valued prediction\n",
    "\n",
    "In this example, 1-Nearest Neighbor function is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem: Curse of Dimensionality\n",
    "\n",
    "For uniform coverage of space, number of training points needed grows exponentially with dimension.  \n",
    "In order to get a kind of uniform coverage of the full space of a training set, we need a number of training samples which is exponential in the dimension of the underlying space.\n",
    "\n",
    "Number of possible 32 x 32 binary images(like images in CIFAR 10):\n",
    "$2^{32\\times32} \\approx 10^{308}$ <br/>\n",
    "Number of elementary particles in the visible universe:\n",
    "$\\approx 10^{97}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(2.0.10) curse of dimensionality ](pictures/2.0.10.jpg)\n",
    "<center>(2.0.10) curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor on raw pixels is seldom used\n",
    "\n",
    "- very slow at test time\n",
    "\n",
    "- it's very difficult to get enough data to cover the space of all possible images\n",
    "\n",
    "- distance metrics on raw pixel values is not very semantically meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But Nearest Neighbor using feature vectors computed from deep convolutional neural networks works well both in image classification and image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In image classification we start with a training set of images and labels, and must predict labels on the test set\n",
    "\n",
    "Image classification is challenging due to the semantic gap: we need invariance to occlusion, deformation, lighting, intraclass variation, etc\n",
    "\n",
    "Image classification is a building block for other vision tasks\n",
    "\n",
    "The K-Nearest Neighbors classifier predicts lables based on nearest training examples\n",
    "\n",
    "Distance metric and K are hyperparameters\n",
    "\n",
    "Choose hyperparameters using the validation set;only run on the test set once at the very end"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fd3918c69f3369c3daf3d578e8e87f300bd3f437878382da31ebea14a766bdb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
